
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">


<link rel="icon" href="./img/nft_1~2.png" sizes="50x50">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

    <head>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

        <title>Peng Xia&#39;s homepage</title>

        

        <!-- CSS -->

        <link href="css" rel="stylesheet" type="text/css">

        <link rel="stylesheet" href="style.css" type="text/css" media="screen">

        <!-- ENDS CSS -->

        <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>

        

        <!-- ENDS JS -->  

    </head> 

    <body>

        <!-- MAIN -->

        <div id="">

            <div id="match-nav-wrapper">

               <div id="match-nav-bar">

                     <table>

                        <thead>

                            <tr valign="bottom">

                                <th width="" style="font-size: 25px;  color: #dddddd">Peng Xia</th>

                               <th width=40% ></th>

                                <th width=""><a href="index.html">HOME |</a> </th>

                                <th width=""><a href="publication.html"> RESEARCH |</a></th>

                                <th width=""><a href="cv.html">CV</a></th>

                            </tr>

                        </thead>

                    </table>

                </div>

            </div>

            <!-- HEADER -->





            <div id="main-wrapper">



                <div id="portfolio-info">

                <h1>üìùPublications & Preprints</h1>

                    <table id="portfolio-projects">
                        <p>* indicates equal contribution; <sup>‚Ä†</sup> indicates corresponding authorship.</p>

                        <tbody>      
                        
                        <tr style="border-width: 1px">
                                <td><img src="./img/fig9.png" style="margin-bottom: 12px" width="180" height="80"></td>
                                <td bgcolor="#e4e4e4">
                                    <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                    <p>
                                         <b> MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models.</b><br>
                                         <b>P. Xia</b>, K. Zhu, H. Li, T. Wang, W. Shi, L. Zhang, J. Zou, H Yao.<br>
                                         arXiv preprint, 2024.  <br> <i>the short version is presented in NeurIPS 2024 Workshop on <a href="https://adaptive-foundation-models.org/">Adaptive Foundation Models</a> and <a href="https://safegenaiworkshop.github.io/">Safe Generative AI</a>. </i>  <br>
                                          <a href="https://arxiv.org/abs/2410.13085">Paper</a> &nbsp;¬∑&nbsp;
                                          <a href="https://github.com/richard-peng-xia/MMed-RAG">Code</a> <a href="https://github.com/richard-peng-xia/MMed-RAG"><img src="https://img.shields.io/github/stars/richard-peng-xia/MMed-RAG?style=social&label=Code+Stars"></img></a>
                                    </p>
                                    </td>
                                    <td style="width: 10px;">
                                    </td>
                                    </tr>
                                    </tbody></table>           
                                </td>
                          </tr> 
                        
                        <tr style="border-width: 1px">
                                <td><img src="./img/fig8.png" style="margin-bottom: 12px" width="180" height="80"></td>
                                <td style="">
                                    <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                    <p>
                                         <b> Massive Multimodal Interleaved Comprehension Benchmark For Large Vision-Language Models.</b><br>
                                         <b>P. Xia</b>*, S. Han*, S. Qiu*, Y. Zhou, Z. Wang, W. Zheng, Z. Chen, C. Cui, M. Ding, L. Li, L. Wang, H Yao.<br>
                                         arXiv preprint, 2024.  <br> <i>the short version is presented in NeurIPS 2024 Workshop on <a href="https://adaptive-foundation-models.org/">Adaptive Foundation Models</a>. </i>  <br>
                                          <a href="https://arxiv.org/abs/2410.10139">Paper</a> &nbsp;¬∑&nbsp;
                                          <a href="https://github.com/Lillianwei-h/MMIE">Code</a> <a href="https://github.com/Lillianwei-h/MMIE"><img src="https://img.shields.io/github/stars/Lillianwei-h/MMIE?style=social&label=Code+Stars"></img></a>
                                    </p>
                                    </td>
                                    <td style="width: 10px;">
                                    </td>
                                    </tr>
                                    </tbody></table>           
                                </td>
                          </tr> 

                        <tr style="border-width: 1px">
                                <td><img src="./img/fig7.png" style="margin-bottom: 12px" width="180" height="80"></td>
                                <td bgcolor="#e4e4e4">
                                    <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                    <p>
                                         <b> RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models.</b><br>
                                         <b>P. Xia</b>*, K. Zhu*, H. Li, H. Zhu, Y. Li, G. Li, L. Zhang, H. Yao.<br>
                                         Conference on Empirical Methods in Natural Language Processing (<b>EMNLP</b>), 2024.  <br>
                                          <a href="https://arxiv.org/abs/2407.05131">Paper</a> &nbsp;¬∑&nbsp;
                                          <a href="https://github.com/richard-peng-xia/RULE">Code</a> <a href="https://github.com/richard-peng-xia/RULE"><img src="https://img.shields.io/github/stars/richard-peng-xia/RULE?style=social&label=Code+Stars"></img></a>
                                    </p>
                                    </td>
                                    <td style="width: 10px;">
                                    </td>
                                    </tr>
                                    </tbody></table>           
                                </td>
                          </tr> 

                        <tr style="border-width: 1px">
                                <td><img src="./img/fig2.png" style="margin-bottom: 12px" width="180" height="60"></td>
                                <td style="">
                                    <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                    <p>
                                         <b> CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models.</b><br>
                                         <b>P. Xia</b>, Z. Chen, J. Tian*, Y. Gong*, R. Hou, Y. Xu, Z. Wu, Z. Fan, Y. Zhou, K. Zhu, W. Zheng, Z. Wang, X. Wang, X. Zhang, C. Bansal, M. Niethammer, J. Huang, H. Zhu, Y. Li, Z. Ge, J. Sun, G. Li, J. Zou, H. Yao.<br>
                                         Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024. <br><i>the short version is presented in ICML 2024 Workshop on <a href="https://icml-fm-wild.github.io/">Foundation Models in the Wild</a>. </i> <br>
                                          <a href="https://arxiv.org/abs/2406.06007">Paper</a> &nbsp;¬∑&nbsp;
                                          <a href="https://github.com/richard-peng-xia/CARES">Code</a> <a href="https://github.com/richard-peng-xia/CARES"><img src="https://img.shields.io/github/stars/richard-peng-xia/CARES?style=social&label=Code+Stars"></img></a>
                                    </p>
                                    </td>
                                    <td style="width: 10px;">
                                    </td>
                                    </tr>
                                    </tbody></table>           
                                </td>
                          </tr> 
                        
                        
                          <tr style="border-width: 1px">
                                <td><img src="./img/fig1.png" style="margin-bottom: 12px" width="180" height="60"></td>
                                <td bgcolor="#e4e4e4">
                                    <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                    <p>
                                         <b> Generalizing to Unseen Domains in Diabetic Retinopathy with Disentangled Representations.</b><br>
                                         <b>P. Xia</b>*, M. Hu*, F. Tang, W. Li, W. Zheng, L. Ju, P. Duan, H. Yao<sup>‚Ä†</sup>, Z. Ge<sup>‚Ä†</sup>.<br>
                                         Medical Image Computing and Computer-Assisted Intervention (<b>MICCAI</b>), 2024. (<span style="color: red;">Early Accept</span>, Top 11%) <br>
                                          <a href="https://arxiv.org/abs/2406.06384">Paper</a> &nbsp;¬∑&nbsp;
                                          <a href="https://github.com/richard-peng-xia/DECO">Code</a> <a href="https://github.com/richard-peng-xia/DECO"><img src="https://img.shields.io/github/stars/richard-peng-xia/DECO?style=social&label=Code+Stars"></img></a>
                                    </p>
                                    </td>
                                    <td style="width: 10px;">
                                    </td>
                                    </tr>
                                    </tbody></table>           
                                </td>
                          </tr> 

                            
                          <!-- <tr style="border-width: 1px">
                            <td><img src="./img/fig5.png" style="margin-bottom: 12px" width="180" height="60"></td>
                            <td style="">
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b>HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding.</b><br>
                                     <b>P. Xia</b>, X. Yu, M. Hu, L. Ju, Z. Wang, P. Duan, Z. Ge.<br>
                                      arXiv preprint, 2023.  <br> <i>the short version is presented in ACL 2024 Workshop on <a href="https://alvr-workshop.github.io">Advances in Language and Vision Research (ALVR)</a>. </i> <br>
                                      <a href="https://arxiv.org/abs/2311.14064">Paper</a> &nbsp;¬∑&nbsp;
                                    <a href="https://github.com/richard-peng-xia/HGCLIP">Code</a> <a href="https://github.com/richard-peng-xia/HGCLIP"><img src="https://img.shields.io/github/stars/richard-peng-xia/HGCLIP?style=social&label=Code+Stars"></img></a>
                                </p>
                                </td>
                                <td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr>  -->
                            
                        
                        <tr style="border-width: 1px">
                            <td><img src="./img/fig6.png" style="margin-bottom: 12px" width="180" height="60"></td>
                            <td style="">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding.</b><br>
                                     M. Hu*, <b>P. Xia*</b>, L. Wang*, S. Yan, F. Tang, Z. Xu, Y. Luo, K. Song, J. Leitner, X. Cheng, J. Cheng, C. Liu, K. Zhou<sup>‚Ä†</sup>, Z. Ge<sup>‚Ä†</sup>.<br>
                                     European Conference on Computer Vision (<b>ECCV</b>), 2024. <br>
                                     <a href="https://arxiv.org/pdf/2406.07471">Paper</a>&nbsp;¬∑&nbsp;
                                    <a href="https://github.com/minghu0830/OphNet-benchmark">Code</a> <img src="https://img.shields.io/github/stars/minghu0830/OphNet-benchmark?style=social&label=Code+Stars"></img></a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr> 
                            
                         
                         <!-- <tr style="border-width: 1px">
                            <td><img src="./img/fig3.png" style="margin-bottom: 12px" width="180" height="60"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-Tailed Multi-Label Visual Recognition. </b><br>
                                    <b>P. Xia</b>, D. Xu, M. Hu, L. Ju, Z. Ge.<br>
                                    ACL 2024 Workshop on <a href="https://alvr-workshop.github.io">Advances in Language and Vision Research (ALVR)</a>. <br>                           
                                    <a href="https://arxiv.org/pdf/2305.04536">Paper</a>&nbsp;¬∑&nbsp;
                                    <a href="https://github.com/richard-peng-xia/LMPT">Code</a> <a href="https://github.com/richard-peng-xia/LMPT"><img src="https://img.shields.io/github/stars/richard-peng-xia/LMPT?style=social&label=Code+Stars"></img></a>       
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr>   -->

                            
                        <!-- <tr style="border-width: 1px">
                            <td><img src="./img/fig4.png" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding.</b><br>
                                     M. Hu*, L. Wang*, S. Yan*, D. Ma*, Q. Ren, <b>P. Xia</b>, W. Feng, P. Duan, L. Ju, Z. Ge.<br>
                                     Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023. <br>
                                     <a href="https://arxiv.org/pdf/2310.13347">Paper</a>&nbsp;¬∑&nbsp;
                                    <a href="https://github.com/minghu0830/NurViD-benchmark">Code</a> <a href="https://github.com/minghu0830/NurViD-benchmark"><img src="https://img.shields.io/github/stars/minghu0830/NurViD-benchmark?style=social&label=Code+Stars"></img></a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr>  -->

                                           

                         <tr style="border-width: 1px">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <br> <br>
                                <h1 style="font-weight: bold;">üé®Patents</h1>
                                <ul>
                                    <li><a href="https://patents.google.com/patent/CN117894475A/en">Prediction device for cognitive function result of patient suffering from atrial fibrillation based on neural network.</a><br>
                                    Z. Wang, C. Jiang, <b>P. Xia</b>, ..., C. Ma.<br>
                                    CN Patent. CN202410108284.X, 2024.<br> 
                                    </li>
                                </ul>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr>

		 <tr style="border-width: 1px">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <ul>
                                    <li>
                                    <a href="https://patents.google.com/patent/CN117593395A/en">Longitudinal fundus image generation method, device, electronic equipment and storage medium.</a><br>
                                    M. Hu, <b>P. Xia</b>, ..., Z. Ge & D. Zhang.<br>
                                    CN Patent. CN202311401646.6, 2023.<br> 
                                    </li>
                                </ul>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr>

				
		     <tr style="border-width: 1px">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <ul>
                                    <li>
                                    <a href="https://patents.google.com/patent/CN116994101A/en">Model training method and device, electronic equipment and storage medium.</a><br>
                                    <b>P. Xia</b>, T. Ma, ..., Z. Ge & D. Zhang.<br>
                                    CN Patent. CN202311275093.4, 2023.<br> 
                                    </li>
                                </ul>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr>
	
	
	              <tr style="border-width: 1px">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <ul>
                                    <li>
                                    <a href="https://patents.google.com/patent/CN117253100A/en">Model training method and device, electronic equipment and storage medium.</a><br>
                                    <b>P. Xia</b>, T. Ma, B. Wang, Z. Ge & D. Zhang.<br>
                                    CN Patent. CN202310882543.X, 2023.<br>
                                    </li>
                                </ul>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr>

                        <tr style="border-width: 1px">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <ul>
                                    <li>
                                    <a href="https://patents.google.com/patent/CN116994100A/en">Model training method and device, electronic equipment and storage medium.</a><br>
                                    <b>P. Xia</b>, T. Ma, ..., Z. Ge & D. Zhang.<br>
                                    CN Patent. ZL202311275090.0, 2023. (Granted)<br>
                                    </li>
                                </ul>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr>

                    <tr style="border-width: 1px">
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <ul>
                                <li>
                                <a href="https://patents.google.com/patent/CN116524580A/en">Anxiety depression detection method, anxiety depression detection device, electronic equipment and storage medium.</a><br>
                                <b>P. Xia</b>, T. Ma, B. Wang, Z. Ge & D. Zhang.<br>
                                CN Patent. CN202310573953.6, 2023.<br>
                                </li>
                            </ul>
                            </td><td style="width: 10px;">
                            </td>
                            </tr>
                            </tbody></table>           
                        </td>
                    </tr>

                    <tr style="border-width: 1px">
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <ul>
                                <li>
                                <a href="https://patents.google.com/patent/CN116245834A/en">Method, device, equipment and medium for determining psychological elasticity level based on fundus image.</a><br>
                                <b>P. Xia</b>, L. Ju,..., Z. Ge & D. Zhang.<br>
                                CN Patent. CN202310129842.6, 2023.<br>
                                </li>
                            </ul>
                            </p>
                            </td><td style="width: 10px;">
                            </td>
                            </tr>
                            </tbody></table>           
                        </td>
                    </tr>

                    <tr style="border-width: 1px">
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <ul>
                                <li>
                                <a href="https://patents.google.com/patent/CN115620384A/en">Model training method, fundus image prediction method and device.</a><br>
                                <b>P. Xia</b>, L. Ju,..., Z. Ge & D. Zhang.<br>
                                CN Patent. ZL202211633628.6, 2022. (Granted)<br>
                                </li>
                            </ul>
                            </td><td style="width: 10px;">
                            </td>
                            </tr>
                            </tbody></table>           
                        </td>
                    </tr>

                    <tr style="border-width: 1px">
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <ul>
                                <li>
                                <a href="https://patents.google.com/patent/CN115590481A/en">Apparatus and computer-readable storage medium for predicting cognitive impairment.</a><br>
                                <b>P. Xia</b>, L. Ju,..., Z. Ge & D. Zhang.<br>
                                CN Patent. ZL202211611022.2, 2022. (Granted)<br>
                                </li>
                            </ul>
                            </td><td style="width: 10px;">
                            </td>
                            </tr>
                            </tbody></table>           
                        </td>
                    </tr>

                    <tr style="border-width: 1px">
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <ul>
                                <li>
                                Article quality discrimination software based on multi-model transfer pre-training.<br>
                                J. Li, <b>P. Xia</b>, K. Zeng, et al.<br>
                                CN Software Copyright. 2022SR0228307. (Granted)<br>
                                </li>
                            </ul>
                            </td><td style="width: 10px;">
                            </td>
                            </tr>
                            </tbody></table>           
                        </td>
                    </tr>

                    <tr style="border-width: 1px">
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <ul>
                                <li>
                                Lane detection system based on cascaded convolutional neural network.<br>
                                J. Li, K. Zeng, <b>P. Xia</b>.<br>
                                CN Software Copyright. 2022SR0248890. (Granted)<br>
                                </li>
                            </ul>
                            </td><td style="width: 10px;">
                            </td>
                            </tr>
                            </tbody></table>           
                        </td>
                    </tr>

                

            </div>

            </div>

        </div>


        <!-- ENDS MAIN -->  

  

 

    

</div></div></body></html>
