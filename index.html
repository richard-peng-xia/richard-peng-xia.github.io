<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="keywords" content="Peng Xia, Xia Peng, UNC-Chapel Hill">
    <meta name="description" content="Peng's Homepage">
    <link href="main.css" media="all" rel="stylesheet">
    <link rel="icon" type="image/png" href="./figures/nft_1~2.png">
    <title>Peng Xia&#39;s homepage</title>
</head>

<body>
    <!-------------------------- Intro----------------->
  <table style="width:100%;background:#FFFFFF;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <h2>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Peng "Richard" Xia (夏鹏)</h2>
              </p>
              <p>I am now working with Prof. <a href="https://scholar.google.com/citations?user=A20BZnQAAAAJ&hl=en">Huaxiu Yao</a> at <a href="https://cs.unc.edu/">Department of Computer Science</a>, <a href="https://www.unc.edu/">UNC-Chapel Hill</a>.
                Before that, I was briefly enrolled (2023.09-2024.05) as a Ph.D. student in <a href="https://www.monash.edu/mmai-group">Monash Medical AI Group</a> at <a href="https://www.monash.edu/">Monash University</a>, advised by A/Prof. <a href="https://scholar.google.com.au/citations?user=Q0gUrcIAAAAJ&hl=en">Zongyuan Ge</a>.
                I got B. Eng degree in AI Experimental Class, <a href="http://scst.suda.edu.cn/">School of Computer Science and Technology</a> at <a href="http://eng.suda.edu.cn/">Soochow University</a> in 2023.</p>
                
                <p>
                  I am deeply intrigued by <span style="font-weight: bold;">large-scale vision-language models </span>. Moreover, my fascination extends to their <span style="font-weight: bold;">practical applications</span>, including healthcare, robotics. My recent research endeavors involve <span style="font-weight: bold;">retrieval-based methods</span>, aiming to play a fundamental role in next-generation vision-language models to improve their factuality, adaptability, and trustworthiness.  </p>

                <p>I am always open to collaboration. Feel free to drop me an e-mail. :-)</p>
               <!-- <p>
                 <strong style="color:red;">I'm on the job market for year 2023-2024 !</strong>
              </p> -->
              <p>
                <span style="font-weight: bold;">Email: </span>richard.peng.xia AT gmail DOT com; pxia AT cs DOT unc DOT edu
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=8OVOf1EAAAAJ"><img src="./figures/google_scholar.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Peng-Xia/2261083308"><img src="./figures/semantic.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://github.com/richard-peng-xia"><img src="./figures/github_s.jpg" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/peng-xia-a98783239/"><img src="./figures/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://x.com/richardxp888"><img src="./figures/x.jpg" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://www.researchgate.net/profile/Peng-Xia-26"><img src="./figures/rg.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://orcid.org/0000-0003-2676-9128"><img src="./figures/orcid.jpeg" height="30px" style="margin-bottom:-3px"></a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="figures/xp.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="figures/xp.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


<!-------------------------- News----------------->
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <ul>
                <li>
                  <p>
                  Dec.2024: Invited talk at <a href="https://cohere.com/research">Cohere For AI</a>, one paper was accepted by COLING 2025, two papers were accepted by AAAI 2025.
                </li>
                <li>
                  <p>
                  Sep.2024: One paper was accepted by NeurIPS 2024 and one paper was accepted by EMNLP 2024.
                  </p>
                </li>
                <li>
                  <p>
                   Jul.2024: One paper was accepted by ECCV 2024.
                  </p>
                </li>
                <li>
                  <p>
                  Jun.2024: Two papers were accepted by MICCAI 2024 and one was <span style="color: red;">early accepted</span>.
                  </p>
                </li>
                <li>
                  <p>
                  Sep.2023: One paper was accepted by NeurIPS 2023.
                  </p>
                </li>
                <li>
                  <p>
                  Aug.2022: Share paper list about <a href="https://github.com/richard-peng-xia/awesome-multimodal-in-medical-imaging">multi-modal learning in medical imaging</a>. <a href="https://github.com/richard-peng-xia/awesome-multimodal-in-medical-imaging"><img src="https://img.shields.io/github/stars/richard-peng-xia/awesome-multimodal-in-medical-imaging?style=social&label=Code+Stars"></img></a>
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>


<!-------------------------- Internship----------------->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Experience</h2>
              <ul>
                <li>
                  <p>
                  <strong>Jan.2023 -- May.2024, Monash Medical AI Group, Monash University</strong>, Melbourne, AU
                  <br> Research Assistant / Ph.D. Student
                </li>
                <li>
                  <p>
                  <strong>Jul.2022 -- Jan.2023, Airdoc Technology Inc</strong>, Shanghai, CN
                  <br>Research Intern
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>

    

   <!-------------------------- Publications----------------->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
    <h2>Selected Publications (<a href="https://scholar.google.com/citations?user=8OVOf1EAAAAJ">Full Publications</a>)</h2> 

    

    <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2410.13085">
                <strong>MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models</strong>
              </a> <br>
                <strong>Peng Xia</strong>, Kangyu Zhu, Hanran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao<br>
                <em>arXiv preprint</em>, 2024. | <a href="https://adaptive-foundation-models.org/">Adaptive Foundation Models</a> and <a href="https://safegenaiworkshop.github.io/">Safe Generative AI</a> Workshop at NeurIPS</em>, 2024
                 [<a href="https://arxiv.org/abs/2410.13085">Paper</a>]
                [<a href="https://github.com/richard-peng-xia/MMed-RAG">Code</a>]
                [<a href="https://www.youtube.com/watch?v=tlxMUlkpsIc">News</a>]
            </td>
        </tr>
    </table>

    <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2412.06141">
              <strong>MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization</strong>
            </a> <br>
              Kangyu Zhu*, <strong>Peng Xia*</strong>, Yun Li, Hongtu Zhu, Sheng Wang, Huaxiu Yao<br>
              <em>arXiv preprint</em>, 2024. 
               [<a href="https://arxiv.org/abs/2412.06141">Paper</a>]
              [<a href="https://github.com/aiming-lab/MMedPO">Code</a>]
          </td>
      </tr>
  </table>

     <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2410.10139">
                <strong>MMIE: Massive Multimodal Interleaved Comprehension Benchmark For Large Vision-Language Models.</strong>
              </a> <br>
              <strong>Peng Xia*</strong>, Siwei Han*, Shi Qiu*, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, Huaxiu Yao<br>
              <em>arXiv preprint</em>, 2024. | <a href="https://adaptive-foundation-models.org/">Adaptive Foundation Models</a> Workshop at NeurIPS</em></em>, 2024
                 [<a href="https://arxiv.org/abs/2410.10139">Paper</a>]
                 [<a href="https://github.com/Lillianwei-h/MMIE">Code</a>]
                 [<a href="https://mmie-bench.github.io/">Project Page</a>]
            </td>
        </tr>
    </table>

    <hr>


    <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2407.05131">
                <strong>RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models</strong>
              </a> <br>
              <strong>Peng Xia*</strong>, Kangyu Zhu*, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao<br>
                <em>The Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>)</em>, 2024
                 [<a href="https://arxiv.org/abs/2407.05131">Paper</a>]
                [<a href="https://github.com/richard-peng-xia/RULE">Code</a>]
                [<a href="https://www.youtube.com/watch?v=SqzuakAnsFc">Talk</a>]
            </td>
        </tr>
    </table>

     <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/pdf/2407.01231">
                <strong>CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models</strong>
              </a> <br>
              <strong>Peng Xia</strong>, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, Wenhao Zheng, Zhaoyang Wang, Xiao Wang, Xuchao Zhang, Chetan Bansal, Marc Niethammer, Junzhou Huang, Hongtu Zhu, Yun Li, Jimeng Sun, Zongyuan Ge, Gang Li, James Zou, Huaxiu Yao<br>
            <em> The Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
                 [<a href="https://arxiv.org/pdf/2407.01231">Paper</a>]
                [<a href="https://cares-ai.github.io/">Code</a>]
                [<a href="https://cares-ai.github.io/">Project Page</a>]
            </td>
        </tr>
    </table>

 <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2406.06384">
                <strong>Generalizing to Unseen Domains in Diabetic Retinopathy with Disentangled Representations </strong>
              </a> <br>
              <strong>Peng Xia</strong>, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, Zongyuan Ge              <br>
                <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (<strong>MICCAI</strong>)</em>, 2024
                 [<a href="https://arxiv.org/abs/2406.06384">Paper</a>]
            </td>
        </tr>
    </table>

    <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/pdf/2405.00077">
                <strong>OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding</strong>
              </a> <br>
              Ming Hu*, <strong>Peng Xia*</strong>, Lin Wang*, Siyuan Yan, Feilong Tang, Zhongxing Xu, Yimin Luo, Kaimin Song, Jurgen Leitner, Xuelian Cheng, Jun Cheng, Chi Liu, Kaijing Zhou, Zongyuan Ge<br>
                <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
                 [<a href="https://arxiv.org/pdf/2405.00077">Paper</a>]
            </td>
        </tr>
    </table>

    <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2311.14064">
                <strong>HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding</strong>
              </a> <br>
                 <strong>Peng Xia</strong>, Xingtong Yu, Ming Hu, Lie Ju, Zhiyong Wang, Peibo Duan, Zongyuan Ge<br>
                 <em>The Conference on Computational Linguistics (<strong>COLING</strong>)</em>, 2025
                 [<a href="https://arxiv.org/abs/2311.14064">Paper</a>] [<a href="https://github.com/richard-peng-xia/HGCLIP">Code</a>] 
            </td>
        </tr>
    </table>

      <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2406.15764">
                <strong>TP-DRSeg: Improving Diabetic Retinopathy Lesion Segmentation with Explicit Text-Prompts Assisted SAM   </strong>
              </a> <br>
              Wenxue Li, Xinyu Xiong, <strong>Peng Xia</strong>, Lie Ju, Zongyuan Ge<br>
              <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (<strong>MICCAI</strong>)</em>, 2024
                 [<a href="https://arxiv.org/abs/2406.15764">Paper</a>]
            </td>
        </tr>
    </table>

      <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://aclanthology.org/2024.alvr-1.3/">
                <strong>LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-tailed Multi-Label Visual Recognition </strong>
              </a> <br>
              <strong>Peng Xia</strong>, Di Xu, Ming Hu, Lie Ju, Zongyuan Ge<br>
                 <em>ALVR Workshop @ Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, 2024
                 [<a href="https://aclanthology.org/2024.alvr-1.3/">Paper</a>] [<a href="https://github.com/richard-peng-xia/LMPT">Code</a>]
            </td>
        </tr>
    </table>
<table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2310.13347">
                <strong>NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding  </strong>
              </a> <br>
              Ming Hu, Lin Wang, Siyuan Yan, Don Ma, Qingli Ren, <strong>Peng Xia</strong>, Wei Feng, Peibo Duan, Lie Ju, Zongyuan Ge<br>
              <em> The Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
                 [<a href="https://arxiv.org/abs/2310.13347">Paper</a>] [<a href="https://github.com/minghu0830/NurViD-benchmark">Code</a>]
            </td>
        </tr>
    </table>

        </tr>
        </tbody></table>

<!-------------------------- Talks----------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
<h2>Invited Talks</h2>
<table id="tbTalks" border="0" width="100%">
    <tr>
        <td width="70%">
             <ul>
             <li>
              <p>
                Dec. 2024: <a href="https://cohere.com/research">Cohere For AI</a>, Reliable Multimodal RAG for Factuality in Medical Vision Language Models. [<a href="https://www.youtube.com/watch?v=SqzuakAnsFc">Video</a>] [<a href="https://cohere.com/events/cohere-for-ai-peng-xia-2024">Cohere Event</a>] [<a href="https://x.com/CohereForAI/status/1861463108465660395">X/Twitter</a>] [<a href="https://www.linkedin.com/feed/update/urn:li:activity:7270119719796383750/">Linkedin</a>]</a>
              </p>
            </li>
            <li>
              <p>
                Oct. 2024: <a href="https://www.aitime.cn/">AI TIME</a>, EMNLP Seminar, Reliable Multimodal RAG for Factuality in Medical Vision Language Models. [<a href="https://www.bilibili.com/video/BV1c2yBYwEaq?spm_id_from=333.788.videopod.sections&vd_source=5958d8e72b0a0f4c589dce4213276465">Video (in Chinese)</a>]
              </p>
            </li>
            <li>
              <p>
                Oct. 2024: <a href="https://nice-nlp.github.io/">NICE-NLP</a>, EMNLP Seminar, Reliable Multimodal RAG for Factuality in Medical Vision Language Models. [<a href="https://www.bilibili.com/video/BV1sMykYCEHx/?spm_id_from=333.999.0.0&vd_source=5958d8e72b0a0f4c589dce4213276465">Video (in Chinese)</a>]</a>
              </p>
            </li>
          </ul>
        </td>
    </tr>
</table>
<br>

    </tr>
    </tbody></table>

<!-------------------------- Awards----------------->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
    <h2>Selected Honors &amp; Awards</h2>
    <table id="tbHonors" border="0" width="100%">
        <tr>
            <td width="70%">
                 <ul>
                 <li>
                  <p>
                    MSRI Graduate Scholarship, MSRI Living Stipend, 2023-2024</a>
                  </p>
                </li>
                <li>
                  <p>
                    Third Place, Shanghai-HK Interdisciplinary Shared Tasks Task 1, 2022
                  </p>
                </li>
                <li>
                  <p>
                    Second Price, The 3rd Huawei DIGIX AI Algorithm Contest, 2021
                  </p>
                </li>
                   <li>
                  <p>
                    Honorable Mention, Mathematics Contest in Modeling, 2021. 
                  </p>
                </li>
              </ul>
            </td>
        </tr>
    </table>
    <br>

        </tr>
        </tbody></table>

     <!-------------------------- Services----------------->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Academic Services</h2>
              <ul>
                <li>
                  <p>
                  Student Volunteer: EMNLP2024
                  </p>
                </li>
                <li>
                  <p>
                  Journal/Conference Reviewer: NeurIPS2024, NeurIPS D&B Track2024, ICML2024-2025, ICLR2025, MICCAI2024, WACV2025, ARR2025, IJCV, TMI
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>

  <!-------------------------- Teaching----------------->
       <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Teaching</h2>
              <ul>
                <li>
                  <p>
                  Teaching Assistant, course name, prof. University, 2025 Spring
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table> -->

  <!-------------------------- Stats----------------->

        <p align="center"><center>
          <a href="https://info.flagcounter.com/0RkS"><img src="https://s05.flagcounter.com/map/0RkS/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>
                <br> 
        <font color="#999999">   &copy; Peng Xia | Last updated: <img src="https://img.shields.io/github/last-commit/richard-peng-xia/richard-peng-xia.github.io" alt="last update" />
              </center> </font></p>

    <!-- <p align="center">
        <font color="#999999">Last update: Dec. 2024</font>
    </p> -->



</body>

</html>