<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="keywords" content="Peng Xia, Xia Peng, UNC-Chapel Hill">
    <meta name="description" content="Peng's Homepage">
    <link href="main.css" media="all" rel="stylesheet">
    <link rel="icon" type="image/png" href="./figures/unc.png">
    <title>Peng Xia&#39;s homepage</title>
</head>

<body>
    <!-------------------------- Intro----------------->
  <table style="width:100%;background:#FFFFFF;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <h2>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Peng "Richard" Xia (夏鹏)</h2>
              </p>
              <p>I am a Ph.D. student at <a href="https://cs.unc.edu/">Department of Computer Science</a>, <img src="./figures/unc.png" height="15px" style="margin-bottom:-3px"></a> <a href="https://www.unc.edu/">The University of North Carolina at Chapel Hill</a> (<a href="https://www.unc.edu/">UNC-Chapel Hill</a>), advised by Prof. <a href="https://scholar.google.com/citations?user=A20BZnQAAAAJ&hl=en">Huaxiu Yao</a>.
                Before that, I was briefly enrolled (2023-2024) as a Ph.D. student at <img src="./figures/monash.png" height="15px" style="margin-bottom:-3px"></a> <a href="https://www.monash.edu/">Monash University</a>, advised by A/Prof. <a href="https://scholar.google.com.au/citations?user=Q0gUrcIAAAAJ&hl=en">Zongyuan Ge</a>.
                <!-- I got B. Eng degree in AI Experimental Class, <a href="http://scst.suda.edu.cn/">School of Computer Science and Technology</a> at <a href="http://eng.suda.edu.cn/">Soochow University</a> in 2023. -->
                Formerly, I worked as a research intern at <img src="./figures/qwen.jpeg" height="15px" style="margin-bottom:-3px"></a> <a href="https://github.com/Alibaba-NLP">Tongyi Lab</a>, <a href="https://www.alibaba.com/">Alibaba</a> and <img src="./figures/msr.png" height="15px" style="margin-bottom:-3px"></a> <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>.
              </p>  
                <p>
                  My research focuses on developing and enhancing intelligent <span style="font-weight: bold;">Multimodal Agents</span> that can effectively perceive, comprehend, and reason from diverse, dynamic data. My work specifically leverages <span style="font-weight: bold;">retrieval-based methods</span> and <span style="font-weight: bold;">multi-step tool interaction</span> to boost the system's reasoning capabilities and decision-making efficiency, with a primary application in medical scenarios.</p>
                <p>I am always open to collaboration. Feel free to drop me an e-mail. :-)</p>
               <p style="border: 1px solid #ff4d4f; background-color: #fff2f0; color: #ff4d4f; padding: 10px; border-radius: 5px;">
                 <!-- <strong style="color:red;"> -->
                  I’m open to 2026 summer internships, feel free to reach out!
                <!-- </strong> -->
              </p>
              <p>
                <span style="font-weight: bold;">Email: </span>richard.peng.xia AT gmail DOT com; pxia AT cs DOT unc DOT edu
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=8OVOf1EAAAAJ"><img src="./figures/google_scholar.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Peng-Xia/2261083308"><img src="./figures/semantic.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://github.com/richard-peng-xia"><img src="./figures/github_s.jpg" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/peng-xia-a98783239/"><img src="./figures/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://x.com/richardxp888"><img src="./figures/x.jpg" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://www.researchgate.net/profile/Peng-Xia-26"><img src="./figures/rg.png" height="30px" style="margin-bottom:-3px"></a> &nbsp/&nbsp
                <a href="https://orcid.org/0000-0003-2676-9128"><img src="./figures/orcid.jpeg" height="30px" style="margin-bottom:-3px"></a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="figures/xp.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="figures/xp.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


<!-------------------------- News----------------->
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <div style="max-height:200px; overflow-y:auto; border:1px solid #ddd; padding:10px; border-radius:8px; background:#fafafa;">
              <ul>
                <li>
                  <p>
                    May.2025: One paper was accepted by ICML 2025.
                  </p>
                </li>
                <li>
                  <p>
                  Jan.2025: Three papers were accepted by ICLR 2025 and <a href="https://mmie-bench.github.io/">MMIE</a> was selected as an <span style="color: red;">oral presentation</span>.
                </li>
                <li>
                  <p>
                  Dec.2024: Invited talk at <a href="https://cohere.com/research">Cohere For AI</a>, one paper was accepted by COLING 2025, two papers were accepted by AAAI 2025.
                </li>
                <li>
                  <p>
                  Sep.2024: One paper was accepted by NeurIPS 2024 and one paper was accepted by EMNLP 2024.
                  </p>
                </li>
                <li>
                  <p>
                   Jul.2024: One paper was accepted by ECCV 2024.
                  </p>
                </li>
                <li>
                  <p>
                  Jun.2024: Two papers were accepted by MICCAI 2024 and one was <span style="color: red;">early accepted</span>.
                  </p>
                </li>
                <li>
                  <p>
                  Sep.2023: One paper was accepted by NeurIPS 2023.
                  </p>
                </li>
                <li>
                  <p>
                  Aug.2022: Share paper list about <a href="https://github.com/richard-peng-xia/awesome-multimodal-in-medical-imaging">multi-modal learning in medical imaging</a>. <a href="https://github.com/richard-peng-xia/awesome-multimodal-in-medical-imaging"><img src="https://img.shields.io/github/stars/richard-peng-xia/awesome-multimodal-in-medical-imaging?style=social&label=Code+Stars"></img></a>
                  </p>
                </li>
              </ul>
              </div>
            </td>
          </tr>
        </tbody></table>


<!-------------------------- Internship----------------->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Experience</h2>
              <ul>
                <li>
                  <p>
                  <strong>Jan.2023 -- May.2024, Monash Medical AI Group, Monash University</strong>, Melbourne, AU
                  <br> Research Assistant / Ph.D. Student
                </li>
                <li>
                  <p>
                  <strong>Jul.2022 -- Jan.2023, Airdoc Technology Inc</strong>, Shanghai, CN
                  <br>Research Intern
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table> -->

    

   <!-------------------------- Publications----------------->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
    <h2>Selected Publications (<a href="https://scholar.google.com/citations?user=8OVOf1EAAAAJ">Full Publications</a>)</h2> 
  
  <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2508.05748">
              <strong>WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent</strong>
            </a> <br>
              Xinyu Geng*, <strong>Peng Xia*</strong>, Zhen Zhang*, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou<br>
              <em>arXiv preprint</em>, 2025. 
               [<a href="https://arxiv.org/abs/2508.05748">Paper</a>]
              [<a href="https://github.com/Alibaba-NLP/WebAgent">Code</a>]
          </td>
      </tr>
  </table>

  <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2507.06229">
              <strong>Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving</strong>
            </a> <br>
              Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, <strong>Peng Xia</strong>, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou<br>
              <em>arXiv preprint</em>, 2025. 
               [<a href="https://arxiv.org/abs/2507.06229">Paper</a>]
              [<a href="https://github.com/OPPO-PersonalAI/Agent-KB">Code</a>]
          </td>
      </tr>
  </table>

  <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2506.00555">
              <strong>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</strong>
            </a> <br>
              <strong>Peng Xia</strong>, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Yan Lu, Huaxiu Yao<br>
              <em>arXiv preprint</em>, 2025. 
               [<a href="https://arxiv.org/abs/2506.00555">Paper</a>]
              [<a href="">Code</a>]
          </td>
      </tr>
  </table>

  <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2503.13964">
              <strong>MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding</strong>
            </a> <br>
              Siwei Han, <strong>Peng Xia</strong>, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, Huaxiu Yao<br>
              <em>arXiv preprint</em>, 2025. 
               [<a href="https://arxiv.org/abs/2503.13964">Paper</a>]
              [<a href="https://github.com/aiming-lab/MDocAgent">Code</a>]
          </td>
      </tr>
  </table>

  <hr>
    <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2412.06141">
              <strong>MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization</strong>
            </a> <br>
              Kangyu Zhu*, <strong>Peng Xia*</strong>, Yun Li, Hongtu Zhu, Sheng Wang, Huaxiu Yao<br>
              <em>International Conference on Machine Learning (<span style="color: blue;"><strong>ICML</strong></span>)</em>, 2025.
              [<a href="https://arxiv.org/abs/2412.06141">Paper</a>]
              [<a href="https://github.com/aiming-lab/MMedPO">Code</a>]
          </td>
      </tr>
  </table>

    <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2410.13085">
                <strong>MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models</strong>
              </a> <br>
                <strong>Peng Xia</strong>, Kangyu Zhu, Hanran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao<br>
                <em>International Conference on Learning Representations (<span style="color: blue;"><strong>ICLR</strong></span>)</em>, 2025. 
                 [<a href="https://arxiv.org/abs/2410.13085">Paper</a>]
                [<a href="https://github.com/richard-peng-xia/MMed-RAG">Code</a>]
                [<a href="https://www.youtube.com/watch?v=tlxMUlkpsIc">Marktechpost Video</a>]
                [<a href="https://www.marktechpost.com/2024/10/19/mmed-rag-a-versatile-multimodal-retrieval-augmented-generation-system-transforming-factual-accuracy-in-medical-vision-language-models-across-multiple-domains/">Marktechpost News</a>]
            </td>
        </tr>
    </table>

     <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2410.10139">
                <strong>MMIE: Massive Multimodal Interleaved Comprehension Benchmark For Large Vision-Language Models.</strong>
              </a> <br>
              <strong>Peng Xia*</strong>, Siwei Han*, Shi Qiu*, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, Huaxiu Yao<br>
              <em>International Conference on Learning Representations (<span style="color: blue;"><strong>ICLR</strong></span>)</em>, 2025. (<span style="color: red;">Oral</span>)
                 [<a href="https://arxiv.org/abs/2410.10139">Paper</a>]
                 [<a href="https://github.com/Lillianwei-h/MMIE">Code</a>]
                 [<a href="https://mmie-bench.github.io/">Project Page</a>]
            </td>
        </tr>
    </table>

    <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2504.19276">
              <strong>AnyPrefer: An Automatic Framework for Preference Data Synthesis.</strong>
            </a> <br>
            Yiyang Zhou*, Zhaoyang Wang*, Tianle Wang*, Shangyu Xing, <strong>Peng Xia</strong>, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao<br>
            <em>International Conference on Learning Representations (<span style="color: blue;"><strong>ICLR</strong></span>)</em>, 2025.
               [<a href="https://arxiv.org/abs/2504.19276">Paper</a>]
          </td>
      </tr>
  </table>

    <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2407.05131">
                <strong>RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models</strong>
              </a> <br>
              <strong>Peng Xia*</strong>, Kangyu Zhu*, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao<br>
                <em>The Conference on Empirical Methods in Natural Language Processing (<span style="color: blue;"><strong>EMNLP</strong></span>)</em>, 2024
                 [<a href="https://arxiv.org/abs/2407.05131">Paper</a>]
                [<a href="https://github.com/richard-peng-xia/RULE">Code</a>]
                [<a href="https://www.youtube.com/watch?v=SqzuakAnsFc">Talk</a>]
            </td>
        </tr>
    </table>

     <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/pdf/2407.01231">
                <strong>CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models</strong>
              </a> <br>
              <strong>Peng Xia</strong>, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, Wenhao Zheng, Zhaoyang Wang, Xiao Wang, Xuchao Zhang, Chetan Bansal, Marc Niethammer, Junzhou Huang, Hongtu Zhu, Yun Li, Jimeng Sun, Zongyuan Ge, Gang Li, James Zou, Huaxiu Yao<br>
            <em> The Conference on Neural Information Processing Systems (<span style="color: blue;"><strong>NeurIPS</strong></span>)</em>, 2024
                 [<a href="https://arxiv.org/pdf/2407.01231">Paper</a>]
                [<a href="https://cares-ai.github.io/">Code</a>]
                [<a href="https://cares-ai.github.io/">Project Page</a>]
            </td>
        </tr>
    </table>

    <!-- <hr> -->
    <!-- <p>During Ph.D. Journey at Monash<a href="https://www.monash.edu/"><img src="./figures/monash.png" height="20px" style="margin-bottom:-3px"></a></p>

    <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2311.14064">
                <strong>HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding</strong>
              </a> <br>
                 <strong>Peng Xia</strong>, Xingtong Yu, Ming Hu, Lie Ju, Zhiyong Wang, Peibo Duan, Zongyuan Ge<br>
                 <em>The Conference on Computational Linguistics (<span style="color: blue;"><strong>COLING</strong></span>)</em>, 2025
                 [<a href="https://arxiv.org/abs/2311.14064">Paper</a>] [<a href="https://github.com/richard-peng-xia/HGCLIP">Code</a>] 
            </td>
        </tr>
    </table>

    <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/pdf/2406.07471">
              <strong>OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding</strong>
            </a> <br>
            Ming Hu*, <strong>Peng Xia*</strong>, Lin Wang*, Siyuan Yan, Feilong Tang, Zhongxing Xu, Yimin Luo, Kaimin Song, Jurgen Leitner, Xuelian Cheng, Jun Cheng, Chi Liu, Kaijing Zhou, Zongyuan Ge<br>
              <em>European Conference on Computer Vision (<span style="color: blue;"><strong>ECCV</strong></span>)</em>, 2024
               [<a href="https://arxiv.org/pdf/2406.07471">Paper</a>] [<a href="https://github.com/minghu0830/OphNet-benchmark">Code</a>] [<a href="https://minghu0830.github.io/OphNet-benchmark/">Project Page</a>]
          </td>
      </tr>
  </table>

    <table id="tbPublications" width="100%">
      <tr>
          <td><a href="https://arxiv.org/abs/2406.06384">
              <strong>Generalizing to Unseen Domains in Diabetic Retinopathy with Disentangled Representations </strong>
            </a> <br>
            <strong>Peng Xia</strong>, Ming Hu, Feilong Tang, Wenxue Li, Wenhao Zheng, Lie Ju, Peibo Duan, Huaxiu Yao, Zongyuan Ge              <br>
              <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (<span style="color: blue;"><strong>MICCAI</strong></span>)</em>, 2024 <span style="color: red;">Early Accepted</span>
               [<a href="https://arxiv.org/abs/2406.06384">Paper</a>]
          </td>
      </tr>
  </table>

      <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2406.15764">
                <strong>TP-DRSeg: Improving Diabetic Retinopathy Lesion Segmentation with Explicit Text-Prompts Assisted SAM   </strong>
              </a> <br>
              Wenxue Li, Xinyu Xiong, <strong>Peng Xia</strong>, Lie Ju, Zongyuan Ge<br>
              <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (<span style="color: blue;"><strong>MICCAI</strong></span>)</em>, 2024
                 [<a href="https://arxiv.org/abs/2406.15764">Paper</a>]
            </td>
        </tr>
    </table>
   
    <table id="tbPublications" width="100%">
      <tr>
          <td><a href="">
              <strong>Towards Realistic Semi-supervised Medical Image Classification </strong>
            </a> <br>
            Wenxue Li, Lie Ju, Feilong Tang, <strong>Peng Xia</strong>, Xinyu Xiong, Ming Hu, Lei Zhu, Zongyuan Ge<br>
            <em>The Conference on Association for the Advancement of Artificial Intelligence (<span style="color: blue;"><strong>AAAI</strong></span>)</em>, 2025
               [<a href="">Paper</a>]
          </td>
      </tr>
  </table>

  <table id="tbPublications" width="100%">
    <tr>
        <td><a href="https://www.arxiv.org/pdf/2412.19871">
            <strong>Neighbor Does Matter: Density-Aware Contrastive Learning for Medical Semi-supervised Segmentation  </strong>
          </a> <br>
          Feilong Tang, Zhongxing Xu, Ming Hu, Wenxue Li, <strong>Peng Xia</strong>, Yiheng Zhong, Hanjun Wu, Jionglong Su, Zongyuan Ge<br>
          <em>The Conference on Association for the Advancement of Artificial Intelligence (<span style="color: blue;"><strong>AAAI</strong></span>)</em>, 2025
             [<a href="https://www.arxiv.org/pdf/2412.19871">Paper</a>]
        </td>
    </tr>
</table>

      <table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://aclanthology.org/2024.alvr-1.3/">
                <strong>LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-tailed Multi-Label Visual Recognition </strong>
              </a> <br>
              <strong>Peng Xia</strong>, Di Xu, Ming Hu, Lie Ju, Zongyuan Ge<br>
                 <em>ALVR Workshop @ Annual Meeting of the Association for Computational Linguistics (<span style="color: blue;"><strong>ACL</strong></span>)</em>, 2024
                 [<a href="https://aclanthology.org/2024.alvr-1.3/">Paper</a>] [<a href="https://github.com/richard-peng-xia/LMPT">Code</a>]
            </td>
        </tr>
    </table>
<table id="tbPublications" width="100%">
        <tr>
            <td><a href="https://arxiv.org/abs/2310.13347">
                <strong>NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding  </strong>
              </a> <br>
              Ming Hu, Lin Wang, Siyuan Yan, Don Ma, Qingli Ren, <strong>Peng Xia</strong>, Wei Feng, Peibo Duan, Lie Ju, Zongyuan Ge<br>
              <em> The Conference on Neural Information Processing Systems (<span style="color: blue;"><strong>NeurIPS</strong></span>)</em>, 2023
                 [<a href="https://arxiv.org/abs/2310.13347">Paper</a>] [<a href="https://github.com/minghu0830/NurViD-benchmark">Code</a>]
            </td>
        </tr>
    </table> -->

        </tr>
        </tbody></table>

<!-------------------------- Talks----------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
<h2>Invited Talks</h2>
<table id="tbTalks" border="0" width="100%">
    <tr>
        <td width="70%">
             <ul>
              <li>
                <p>
                Jul. 2025: <a href="https://www.techbeat.net/">TechBeat</a>, MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models. [<a href="https://www.techbeat.net/talk-info?id=982">Video (in Chinese)</a>]
                </p>
              </li>
              <li>
                <p>
                Apr. 2025: <a href="https://iclr.cc/Conferences/2025">ICLR Oral Session</a>, Massive Multimodal Interleaved Comprehension Benchmark For Large Vision-Language Models. 
                </p>
              </li>
             <li>
              <p>
                Dec. 2024: <a href="https://cohere.com/research">Cohere For AI</a>, Reliable Multimodal RAG for Factuality in Medical Vision Language Models. [<a href="https://www.youtube.com/watch?v=SqzuakAnsFc">Video</a>] [<a href="https://cohere.com/events/cohere-for-ai-peng-xia-2024">Cohere Event</a>] [<a href="https://x.com/CohereForAI/status/1861463108465660395">X/Twitter</a>] [<a href="https://www.linkedin.com/feed/update/urn:li:activity:7270119719796383750/">Linkedin</a>]</a>
              </p>
            </li>
            <li>
              <p>
                Oct. 2024: <a href="https://scst.suda.edu.cn">School of Computer Science, Soochow University</a>, Reliable Multimodal RAG in Medical Vision Language Models. 
              </p>
            </li>
            <li>
              <p>
                Oct. 2024: <a href="https://www.aitime.cn/">AI TIME</a>, EMNLP Seminar, Reliable Multimodal RAG in Medical Vision Language Models. [<a href="https://www.bilibili.com/video/BV1c2yBYwEaq?spm_id_from=333.788.videopod.sections&vd_source=5958d8e72b0a0f4c589dce4213276465">Video (in Chinese)</a>]
              </p>
            </li>
            <li>
              <p>
                Oct. 2024: <a href="https://nice-nlp.github.io/">NICE-NLP</a>, EMNLP Seminar, Reliable Multimodal RAG in Medical Vision Language Models. [<a href="https://www.bilibili.com/video/BV1sMykYCEHx/?spm_id_from=333.999.0.0&vd_source=5958d8e72b0a0f4c589dce4213276465">Video (in Chinese)</a>]</a>
              </p>
            </li>
          </ul>
        </td>
    </tr>
</table>
<br>

    </tr>
    </tbody></table>

<!--------------------------Patents----------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
<h2>Patents</h2>
<table id="tbTalks" border="0" width="100%">
    <tr>
        <td width="70%">
             <ul>
             <li>
              <p>
                <a href="https://patents.google.com/patent/CN116994100A/en?oq=CN116994100A">Model Training Method, Device, Electronic Device and Storage Medium</a> <br>
                <b>Peng Xia</b>, Tong Ma, Ming Hu, Lie Ju, Bin Wang, Zongyuan Ge, Dalei Zhang <br> 
                CN Patent, CN116994100A, 2023.
              </p>
            </li>
            <li>
              <p>
                <a href="https://patents.google.com/patent/CN115620384A/en?oq=CN115620384A">Model Training Method, Fundus Image Prediction Method and Device</a> <br>
                <b>Peng Xia</b>, Lie Ju, Ming Hu, Tong Ma, Bin Wang, Kaimin Song, Zongyuan Ge, Dalei Zhang <br> 
                CN Patent, CN115620384A, 2023.
              </p>
            </li>
            <li>
              <p>
                <a href="https://patents.google.com/patent/CN115590481A/en?oq=CN115590481A">Apparatus and Computer-Readable Storage Medium for Predicting Cognitive Impairment</a> <br>
                <b>Peng Xia</b>, Tong Ma, Ming Hu, Lie Ju, Bin Wang, Zongyuan Ge, Dalei Zhang <br> 
                CN Patent, CN115590481A, 2022.
              </p>
            </li>
          </ul>
        </td>
    </tr>
</table>
<br>

    </tr>
    </tbody></table>

<!-------------------------- Press----------------->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
<h2>Press</h2>
<table id="tbTalks" border="0" width="100%">
    <tr>
        <td width="70%">
             <ul>
              <li>
                <p>
                  "WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent" was covered by <a href="https://mp.weixin.qq.com/s/3gzb5QcJ8AO-1EDeECUFlQ">量子位</a>, <a href="https://x.com/_akhaliq/status/1955616103113298142">AK</a>, <a href="https://soessentially.substack.com/p/webwatcher-wins">So Essentially</a>, <a href="https://mail.bycloud.ai/p/how-ai-is-learning-to-reason-rl-tricks-policy-optimization-and-the-new-webwatcher-agent-ad29f2fed700">BYCLOUD AI</a>, <a href="https://www.emergentmind.com/topics/deep-research-agent">Beehiiv</a>.
                </p>
              </li>
              <li>
              <p>
                "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving" was covered by <a href="https://pub.towardsai.net/i-trained-my-agent-to-learn-from-each-other-and-it-blew-my-mind-b6621cfdd970">Medium Towards AI</a>, <a href="https://mp.weixin.qq.com/s/kyWxNcPsRBns-0QtuIWdrA">机器之心</a>.
              </p>
            </li>
             <li>
              <p>
                MMed-RAG: A Versatile Multimodal Retrieval-Augmented Generation System Transforming Factual Accuracy in Medical Vision-Language Models Across Multiple Domains" was covered by <a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508201033-Yao.html">Banff International Research Station</a>, <a href="https://www.marktechpost.com/2024/10/19/mmed-rag-a-versatile-multimodal-retrieval-augmented-generation-system-transforming-factual-accuracy-in-medical-vision-language-models-across-multiple-domains/">MarkTechPost</a>, <a href="https://www.themoonlight.io/en/review/mmed-rag-versatile-multimodal-rag-system-for-medical-vision-language-models">Moonlight Press</a>, <a href="https://www.youtube.com/watch?v=ggWdYSqBRiM">neptune.ai</a>.
              </p>
            </li>
          </ul>
        </td>
    </tr>
</table>
<br>

    </tr>
    </tbody></table>

<!-------------------------- Awards----------------->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
    <h2>Selected Honors &amp; Awards</h2>
    <table id="tbHonors" border="0" width="100%">
        <tr>
            <td width="70%">
                 <ul>
                  <li>
                    <p>
                      KDD 2025 Health Day Distinguished Vision Award, 2025
                    </p>
                  </li>
                  <li>
                    <p>
                      ICLR Travel Award, 2025
                    </p>
                  </li>
                  <li>
                    <p>
                      ICLR Oral Presentation (Top 1.8%), 2025
                    </p>
                  </li>
                <li>
                  <p>
                    Third Place, Shanghai-HK Interdisciplinary Shared Tasks (Task 1), 2022
                  </p>
                </li>
                <li>
                  <p>
                    Second Price, The 3rd Huawei DIGIX AI Algorithm Contest, 2021
                  </p>
                </li>
                   <li>
                  <p>
                    Honorable Mention, Mathematics Contest in Modeling, 2021
                  </p>
                </li>
              </ul>
            </td>
        </tr>
    </table>
    <br>

        </tr>
        </tbody></table>

     <!-------------------------- Services----------------->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Academic Services</h2>
              <ul>
                <li>
                  <p>
                  Area Chair: ACL Rolling Review (ARR) (2025)
                  </p>
                </li>
                <li>
                  <p>
                  Journal/Conference Reviewer: NeurIPS (2024-2025), NeurIPS D&B Track (2024-2025), ICML (2024-2025), ICLR (2025), CVPR (2025), ICCV (2025), AAAI (2026), MICCAI (2024-2025), WACV (2025-2026), ACL Rolling Review (ARR) (2024-2025), International Journal of Computer Vision (IJCV), IEEE Transactions on Medical Imaging (TMI), Knowledge-Based Systems (KBS), Expert Systems with Applications (ESWA)
                  </p>
                </li>
                <li>
                  <p>
                  Student Volunteer: EMNLP (2024)
                  </p>
                </li>
                <li>
                  <p>
                    Workshop Co-Organizer: <a href="https://r2-fm.github.io/">ICML 2025 Workshop on Reliable and Responsible Foundation Models</a>
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>

  <!-------------------------- Teaching----------------->
       <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Teaching</h2>
              <ul>
                <li>
                  <p>
                  Teaching Assistant, course name, prof. University, 2025 Spring
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table> -->

  <!-------------------------- Stats----------------->

        <p align="center"><center>
          <a href="https://info.flagcounter.com/0RkS"><img src="https://s05.flagcounter.com/map/0RkS/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>
                <br> 
        <font color="#999999">   &copy; Peng Xia | Last updated: <img src="https://img.shields.io/github/last-commit/richard-peng-xia/richard-peng-xia.github.io" alt="last update" />
              </center> </font></p>

    <!-- <p align="center">
        <font color="#999999">Last update: Dec. 2024</font>
    </p> -->



</body>

</html>